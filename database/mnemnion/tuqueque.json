{
  "name": "tuqueque",
  "owner": "mnemnion",
  "repo": "tuqueque",
  "description": "A lock-free MPSC queue based on swap buffers",
  "type": "package",
  "topics": [
    "lock-free",
    "mpsc",
    "mpsc-queue",
    "queues",
    "zig",
    "zig-package"
  ],
  "stars": 6,
  "forks": 0,
  "watchers": 1,
  "updated_at": "2025-12-28T21:49:01Z",
  "minimum_zig_version": "0.15.2",
  "readme": "# Tuqueque\n\nThis module provides `TuQueQue`[^†], a lock-free, fallible MPSC queue\nof fixed size.  Lock-free means that writes and reads both proceed\nwithout the use of a mutex, or any other lock of indefinite duration.\nFallible means that enqueues and dequeues can both fail.  MPSC is\nmultiple producer, single consumer: many writers and only one reader.\nFixed size is why writes can fail.  Writes are why dequeues can fail.\n\nThe mechanism is simple and efficient: swap buffers.  At any given\ninstant, one buffer is the write queue (the enqueue), the other is the\nread queue (the dequeue).  Writers contend to reserve queue space,\nusing a single atomic word, write to their reservation, then commit\nthe write length.  This allows them to leapfrog each other — until the\nqueue is full.\n\nAt any instant, the reader (singular!) can dequeue, by swapping which\nqueue is which.  Writes may not be committed, in which case the reader\nmust wait until they are.  This can time out, but in the absence of\nenqueue-side use bugs, the dequeue will eventualy succeed, and no\nfurther writes will be reserved to that side after a swap.\n\nOnce it succeeds, the reader has a slice of whatever the queue carries.\nIt can do reader stuff with the slice, and (after reading, please!)\nresize the dequeue half of the tuqueque, if desired.  It is not a\ncorrectness issue for the halves to be unbalanced, temporarily or\notherwise.\n\nTuned appropriately, this is about as good as it gets.  All common\nmemory is handled atomically, and cache-isolated to eliminate false\nsharing.\n\nThe most important invariant is that there **must** _only_ be one\nreader.  To facilitate this, writes are expected to proceed using a\n`WriteHandle`, a type-erased pointer to the queue exposing only the\nwrite-appropriate functions (and a bit of bookkeeping, it's two words\nwide).\n\nThe library maintains a read lock and will panic in safe modes if two\nreader-side functions ever overlap.  Seriously, don't do it.  There are\nmore ways to get the queue into an inconsistent state that way than the\nassertions can detect and surface.\n\nThis library has 100% line coverage, but is hot off the presses and yet\nto be used in anger[^‡].  Caveat hacker.\n\n## Inspiration\n\n`TuQueQue` is a port of a Rust library, [`swap-buffer-queue`][sbq],\nmany thanks to @wyfo!  It makes some different choices, in a way which\nwill be comprehensible to anyone familiar with both languages: the Zig\nversion puts somewhat more responsibility for correct use in the hands\nof the user, and is able to pick up some small efficiencies in return.\nIn principle, at least: in addition to `NO WARRANTY`, `tuqueque` comes\nwith no benchmarks.\n\n## Installation\n\n`TuQueQue` requires Zig `0.15.2`.\n\n```sh\nzig fetch --save \"https://github.com/mnemnion/tuqueque/archive/refs/tags/v0.0.1.tar.gz\"\n```\n\n## Use\n\nIt's a queue, you queue things with it.\n\n```zig\ntest MyQueue {\n    var q = try MyQueue.create(allocator);\n    errdefer q.errDestroy(allocator); // Doesn't check for e.g. closed queue\n    q.open();\n    q.enqueueValue('H') catch {}; // {} is two kinds of not-enough-room\n    q.enqueueSlice(\"ello!\") catch {};\n    try testing.expectEqualStrings(\"Hello!\", (q.dequeueAvailable() catch unreachable).?);\n    q.close();\n    q.destroy(allocator); // Does check\n}\n```\n\nUnless resizing one side or the other of the queues, no allocation is\nperformed while the queue is in use.  Queues have object identity[^✢] and\nwill be pinned addresses when Zig gains this ability.\n\nNotice that the queue is created in the closed state, and must be opened\nbefore writes will be accepted.\n\n**Only one reader**.\n\n### Reading\n\nA simple read loop might look something like this:\n\n```zig\ndequeue: while (true) {\n    if (reader.q.dequeueAvailable()) |maybe_msgs| {\n        if (maybe_msgs) |msgs| {\n            for (msgs) |msg| {\n                reader.doMessageThing(msg);\n            }\n            continue :dequeue;\n        } else {\n            // Nothing in the queue at time-of-check\n            reader.doSomethingElse();\n            if (reader.done()) break :dequeue;\n            continue :dequeue;\n        }\n    } else |err| {\n        // Means writes haven't completed in time\n        assert(err == error.DequeuePending);\n        sleep(10);\n        continue :dequeue;\n    }\n}\n```\n\nOr, with reader notification configured (the default):\n\n```zig\ndequeue: while (true) {\n    if (reader.q.dequeueWithTimeout(1000) catch continue :dequeue) |msgs| {\n        for (msgs) |msg| {\n            reader.doMessageThings(msg);\n        }\n        if (reader.done()) break :dequeue;\n    } // Nothing after one ms and two tries\n}\n```\n\nThere is also `q.dequeueWait()`, which will block until it sees\nmessages.  Remember that it's always possible to check the queue, find\nnothing, and block, while missing the notification from a concurrent\nwrite!  Timeouts are to be preferred.  A writer can also ping the reader\nwith `w.notifyReader()`, and vice versa.\n\nWriter notification is another configurable option (off by default),\nbut no \"write or block\" methods are provided.  This is an MPSC system,\nwhich assumes many producers.  Blocking would be easy, but it would\nmost likely be wrong as well.  Thundering herds have a tendency to make\nbottlenecking worse than it already is. `q.writeNotifier()` will return\na `Notifier`, a simple `Mutex`/`Condition` pair, which can be used to\nbuild a more responsible write-awaiter, if desired.\n\nWhen you dequeue a slice, it's good for precisely until the next call to\na `dequeue` function, after which, generally, it's getting stomped on by\nwriters.  If you need to put some of the slice back, you can, keep track\nof the index:\n\n```zig\ndequeue: while (true) {\n    if (reader.q.dequeueWithTimeout(1000) catch continue :dequeue) |msgs| {\n        for (msgs, 0..) |msg, i| {\n            if (reader.doInterruptableMessageThings(msg)) |interrupt| {\n                reader.q.requeueAtIndex(i);\n                // be right back\n                return reader.dealWithInterrupt(interrupt);\n            }\n        }\n        if (reader.done()) break :dequeue;\n    }\n}\n```\nYou get the rest of it back next time you call `dequeue`, after that it\nswaps again.  Unless you keep doing it.\n\n### Writing\n\nAs shown, the reader (owner of the queue) can also write to it, but this\nis not the ordinary course of events.\n\n```zig\nvar w = q.writeHandle();\ndefer w.release();\n```\n\nCreates a `WriteHandle`.  This is an erased pointer to the queue, which\nforwards the enqueue-side functions only.  It cannot open, close, nor\nresize, nor, Heaven forfend, dequeue the queue.  It can, however,\nenqueue, and that in several ways.\n\nIf reference counting is enabled (by default, no), a count of\n`writeHandle` calls is kept, and `w.release()` will decrement it.\n`q.destroy()` (but not `q.errDestroy()`) will panic if this count is not\nzero.  This is intended as a useful building block, and sanity check,\nbut is not a complete system to take down a multithreaded program in an\norderly manner, nor even necessarily a part of one.\n\nMost functions which are configuration-specific are `@compileError`s to\ncall when not so configured. `release()` is always allowed, and will\nset the handle pointer to `undefined` (for modes where this has an\neffect) in all cases.  Its use is recommended whenever a `WriteHandle`\nis disposed, as documentation if nothing else.\n\nIn addition to the already-illustrated value and slice writes, a\n`WriteHandle` has `enqueueSliceUpTo`.  This is passed a slice, and\nenqueues as much of it as there's room for, returning what's left over:\nthis will be an empty slice when all values are enqueued.  It will throw\n`QueueIsFull` if unable to enqueue any value at all.\n\nThis variation has the nice property that it will eventually enqueue all\nvalues of a slice larger than the enqueue itself, where `enqueueSlice`\nwill repeatedly fail[^※].  As such, the `UpTo` vesion should be\npreferred for any collection of values where the upper bound is unknown.\nJust remember that the iteration terminates on an empty slice, not `void`\nor `null`.\n\nFor some applications, the writers may wish to write directly to\nthe queue, rather than creating and sending a copy.  For this, use\n`reserveSlice`, which (on success) returns a slice of the queue of the\nrequested slice, then write to it and perform a `commit()`:\n\n```zig\nif (wh.reserveSlice(12)) |slice| {\n    defer wh.commit();\n    @memcpy(slice, \"Hello there!\");\n} else |_| {\n    // Not enough room, it's fine, I didn't\n    // even want to say hello... baka\n}\n```\n\nIt is asserted that any given `WriteHandle` will not open another\nreservation until the last one is committed.\n\nAccomplishing the commit uses some space on the `WriteHandle`, and as\nsuch, the reader side cannot call this function directly.  It's cheap to\ncreate a `WriteHandle` (`defer wh.release()`!) and use it, in the event\nthat the reader needs to do this kind of thing itself.\n\nIf your `TuQueQue` is of `u8`, this slice can be fed to an `Io.Writer`\nfor serialization, or other nefarious purposes.  Please note that there\nis no way to give back any of the reservation, so if the length of the\nwrite is unclear, some protocol must be established with the reader to\nindicate the unused portion.  Zeroes perhaps.\n\nSpeaking of which!  This function can leak the prior contents of the\nqueue, which for some applications is a security risk.  If yours is\namong them, configuring with `.secure_erase = true` will fill the\nreservations with zeroes before handing it over.  This is, by default,\n`false`, under two premises: expense should only be incurred when value\nis obtained, and my users can be expected to RTFM and code responsibly.\nDon't make me regret it.  The criterion is not \"my code doesn't leak\",\nit's \"there are no security implications for queue leakage whatsoever\".\n\nLet us observe that reserving a slice is a _grave_ responsibility.\nWithout the call to `commit()`, the reader will be stuck forever,\nlooping on `DequeuePending` and questioning its life choices.\n\nDon't bum the reader out, writers.  Commit.\n\n## Cleanup\n\nAs mentioned, the queue must be closed before `destroy` is called, and\nall refcounts (when applicable) released.  After closing, writes may be\nongoing, but no new writes will be accepted.  After `dequeueAvailable()`\nreturns `null`, the only way the reader will see more writes is if it\ncalls `open`.\n\nBe careful here!  Writers examine the queue itself to see if it's\nclosed, so some method _must_ be used to ensure that writers have\ndisposed of that capability, or at least won't use it, before the\nqueue is destroyed.  Reference counting is one of several ways to\narrange this.\n\n## Suitability and Correct Use\n\nThe most important invariant is that this is a _single-consumer_ queue.\nThe queue itself is assumed to be owned by the reader, that reader must\nlive on exactly one thread at any given point.  This also circumscribes\nthe boundary within which using `tuqueque` might be a good choice.  No\ndata structure can do everything, and this is not naturally suited to\nwork-stealing, or other approaches to handling the case where a reader\nis unable to keep up with writes.  It's MPSC, and should be used when\nthat pattern is correct for the application.\n\nAnother thing to be aware of is that, at any given time, only half of\nthe capacity is available to the writers at any given point.  The upside\nis that the reader gets all the updates at once on swap, all lined up in\na nice slice with no contention, no pointer chasing, and a predictable\naccess pattern.  The sentiment is that on modern architectures, leaving\nsome space fallow in exchange for minimal waits and blocks is a great\ntrade-off to make.  This data structure was chosen for a message-passing\narchitecture, where the size of each queue half is expected to be more\nthan adequate, with threads able to keep up with the load, and writers\nhaving other things to occupy their scheduled time slices if a queue is\ntemporarily full.  The design emphasizes throughput, and is willing to\ntolerate some latency variance accordingly.\n\nAny single-consumer system will start to behave poorly if the reader\ncannot keep up with the incoming work.  It's straightforward to track\nqueue capacity against messages-per-dequeue and grow accordingly, it\nis not straightforward at all to jimmy in some mechanism to farm out\nwork if growth is consistent.  For 'bursty' workloads, especially ones\nwhich move housekeeping out of the hot path (examples: logging, database\ntransactions), it could perform very well.\n\nIf you think in terms of the derivative, the size of the queue is of\nminor importance.  Either the reader finishes a half before the other\nfills (reads > writes), it's about the same (reads ≅ writes, the\n'butter zone'), or the queue fills and blocks (writes > reads), and\nyou're in trouble, unless that condition is temporary.  Longer queues\nbuy you some flexibility, but the governing equation is as simple as it\nis stern.\n\nA `TuQueQue` queque is quite well-suited to SPSC work, which is after\nall just a special case of MPSC where `M == S`.  Just don't rely on the\nbuilt-in `Notifier`s alone, it takes more than that to avoid deadlock\nraces[^✤].  If you're certain the workload will be SPSC, it may be\nthat something more specialized to that purpose will suit your needs\nbetter.  Then again, maybe not!  It's very good at what it does.\n\nEvery atomic value in a `TuQueQue` is isolated on its own cache line,\nto prevent false sharing.  The queues are also cache-aligned, one\nconsequence is that your starting capacity might be somewhat larger than\nwhat was requested[^✽] .  Writes can occur into the same cache line\nhowever, in fact this would be normal for small writes, and depending\non specifics this might noticeably impact performance.  This can be\navoided, when suitable, by padding the data type used out to fill its\nown cache line.  Since the queues themselves always start at the start\nof a cache line, this will work.\n\nOn some ARM systems (not Apple ones) the padding needed for effective\nCAS operations is larger than the cache line.  This can be configured\nwith the build option `custom_alignment`.  Search string: \"exclusive\nreservation granule\".  While we're on the subject: this library requires\na CPU capable of atomic operations on 64 bits.  The Zig compiler might\nenforce this but I haven't tried it.\n\nThere are a whole host of queries included, answering questions about\ncapacity, messages available, and so on.  These are intended primarily\nas debugging aids, also useful for orderly startup and shutdown.  Using\nthem in-flight produces read pressure on critical atomic values, and\nthe right way to find out how many messages you can get is to dequeue\nthem.  It's no great challenge to track the capacity of the queue halves\nlocally, detect the case of most or all of it getting used, then employ\na simple state machine to grow / shrink both halves over the ordinary\ncourse of message processing.\n\nJust, think of it this way: if you grow a queue to catch more writes,\nthat's debt.  The reader already has more than it can handle, more\nor less by definition, and if that keeps up, the queue will just\nkeep growing, and things will go pear-shaped in the fulness of time.\nConsider making `QueueIsFull` errors a write-side problem to deal with.\n\n[^†]: Like the cognitive fallacy.  Pronounced _tu-**kyu**-kway_, if\npossible.  You can also say _¿Tu que que?_, but you had better know who\nyou're saying it to.\n\n[sbq]: https://github.com/wyfo/swap-buffer-queue\n\n[^‡]: Likely this will no longer be true well before the next time the\nREADME gets updated.  I did not write this library as an intellectual\nexercise.\n\n[^✢]: The implementation uses internal self-pointers.  Copying a queue\ndoesn't give you a queue clone, it gives you a serious headache later\nwhen you try and figure out why things have gone wrong.\n\n[^※]: There is no distinct error for an attempt to enqueue a slice so\nlarge that it's guaranteed to fail.  Enqueuing capacity is a mutable\nproperty: the dequeue might be resizing at the time of query, or already\nlarger, and so on, so this doesn't strike me as an actually-distinct\ncase from `error.QueueIsFull`.  Corollary: if you're not sure, use\n`enqueueUpToSlice`.\n\n[^✤]: Just as true for MPSC, but somehow less tempting.  Threads are\nnot coroutines!\n\n[^✽]: Resizes get exactly what they ask for, for sane book-keeping.\nA test build will do likewise for the same reason.  Just know that\nrounding up to the cache-line boundary is free, in that any sane\nallocator will round the allocation at least to the alignment, so the\nslice you get back may as well access the full allocation.\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/231785?u=8a0cdeb97b0a614b0350536ecd6921ee6166c611&v=4",
  "releases": [
    {
      "tag_name": "v0.0.1",
      "name": "TuQueQue",
      "body": "A lock-free MPSC queue built with swap buffers.",
      "prerelease": false,
      "published_at": "2025-12-24T16:18:13Z",
      "html_url": "https://github.com/mnemnion/tuqueque/releases/tag/v0.0.1",
      "assets": []
    }
  ],
  "owner_company": null,
  "owner_location": "Eastern Standard Tribe",
  "owner_blog": null,
  "owner_twitter_username": null,
  "owner_followers": 94,
  "owner_following": 24,
  "owner_created_at": "2010-03-27T23:31:39Z",
  "category": "library"
}