{
  "name": "Zorch",
  "owner": "SuperKogito",
  "repo": "Zorch",
  "description": "Neural networks in Zig ",
  "type": "project",
  "topics": [
    "autograd",
    "automatic-differentiation",
    "deep-learning",
    "machine-learning",
    "neural-network",
    "tensor",
    "zig",
    "zig-library",
    "zig-package",
    "ziglang"
  ],
  "stars": 5,
  "forks": 0,
  "watchers": 1,
  "updated_at": "2025-08-26T02:20:34Z",
  "dependencies": [
    {
      "name": "example",
      "url": "https://example.com/foo.tar.gz",
      "hash": "..."
    }
  ],
  "minimum_zig_version": "0.11.0",
  "readme": "![](media/zorch.png?raw=true)\n\n\n# Zorch: A Tensor Library with a Pytorch-like API in Zig\n\nZorch is a lightweight, high-performance tensor library written in Zig. It provides a flexible and efficient framework for numerical computations, automatic differentiation, and machine learning. The library is designed to be simple, modular, and easy to extend.\n\n\n***NB: The library is still experimental and not stable enough***\n\n#  Table of Contents\n\n- [Zorch: A Tensor Library with a Pytorch-like API in Zig](#zorch-a-tensor-library-with-a-pytorch-like-api-in-zig)\n- [Table of Contents](#table-of-contents)\n  - [Features and To-dos](#features-and-to-dos)\n    - [Tensor, Ndarray](#tensor-ndarray)\n    - [Autograd](#autograd)\n    - [Neural Networks](#neural-networks)\n    - [Optimization](#optimization)\n    - [Datasets](#datasets)\n    - [Utilities](#utilities)\n    - [Testing](#testing)\n    - [Documentation](#documentation)\n  - [Project Structure](#project-structure)\n  - [Getting Started](#getting-started)\n    - [Prerequisites](#prerequisites)\n    - [Building the Project](#building-the-project)\n  - [Documentation](#documentation-1)\n  - [Examples](#examples)\n    - [Creating a Tensor](#creating-a-tensor)\n    - [Performing Tensor Operations](#performing-tensor-operations)\n    - [Using Automatic Differentiation](#using-automatic-differentiation)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n## Features and To-dos\n\n### Tensor, Ndarray\n- [x] `Tensor, Ndarray` struct\n- [x] `Tensor, Ndarray` creation (`from_value`, `from_data`, `zeros`, `ones`, `random`)\n- [x] `Tensor, Ndarray` addition (`add`, `add_scalar`)\n- [x] `Tensor, Ndarray` subtraction (`sub`, `sub_scalar`)\n- [x] `Tensor, Ndarray` multiplication (`mul`, `mul_scalar`)\n- [x] `Tensor, Ndarray` division (`div`, `div_scalar`)\n- [x] `Tensor, Ndarray` power (`pow`, `pow_scalar`)\n- [x] `Tensor, Ndarray` matrix multiplication (`matmul`)\n- [x] `Tensor, Ndarray` reshaping (`reshape`)\n- [x] `Tensor, Ndarray` slicing (`slice`)\n- [x] `Tensor, Ndarray` broadcasting (`broadcast_to`)\n- [x] `Tensor, Ndarray` element-wise operations (`equal`, `greater_than`, `less_than`)\n- [x] `Tensor, Ndarray` reduction operations (`sum`, `mean`, `min`, `max`, `argmin`, `argmax`)\n- [x] `Tensor, Ndarray` activation functions (`relu`, `tanh`, `sigmoid`, `softmax`)\n- [x] `Tensor, Ndarray` logging and printing (`print`, `info`)\n- [x] `Tensor, Ndarray` broadcasting support (`broadcast_to`)\n- [ ] Support for sparse tensors\n- [ ] Support for GPU acceleration\n- [ ] Support for BLAS acceleration\n- [ ] Concatenation and stacking operations\n\n### Autograd\n- [x] Backpropagation for (`addition`, `multiplication`, `substraction`, `division`, etc.)\n- [x] Backpropagation for matrix multiplication\n- [x] Gradient accumulation\n- [x] Zeroing gradients (`zero_grad`)\n- [ ] Support caching between forward and backward functions\n- [ ] Backpropagation for more operations (e.g., division, power)\n- [ ] Support for higher-order derivatives\n- [ ] Memory optimization for computation graphs\n- [ ] Memory optimization for computation graphs\n\n### Neural Networks\n- [x] Linear layer\n- [ ] Convolution layers (`Conv2D`, `Conv1D`, etc.)\n- [ ] Pooling layers (`MaxPool`, `MeanPool`, etc.)\n- [ ] Recurrent layers (`RNN`, `LSTM`, etc.)\n- [ ] Loss functions (e.g., CrossEntropy, MSE)\n\n### Optimization\n- [x] Stochastic Gradient Descent (SGD)\n- [ ] Learning rate scheduling\n- [ ] Implement more optimizers (e.g., Adam, RMSprop)\n- [ ] Learning rate schedulers (e.g., StepLR, ReduceOnPlateau)\n\n### Datasets\n- [x] Generate XOR training dataset\n- [ ] Parse MNIST dataset\n\n### Utilities\n- [x] Custom error handling (`TensorError`, `NdarrayError`)\n- [x] Logging with timestamps and colors\n- [x] Data type conversion (`convert_value_to_dtype`)\n\n### Testing\n- [x] Unit tests for most modules\n- [ ] Add unit tests for all modules\n- [ ] Add integration tests for end-to-end workflows\n  \n### Documentation\n- [x] Inline docstrings for all functions\n- [x] Generated HTML documentation\n- [ ] Add more examples and tutorials\n- [ ] Improve inline documentation\n\n\n\n## Project Structure\n\nThe project is organized as follows:\n\n```\n.\n├── build.zig            # Build configuration for Zig\n├── build.zig.zon        # Dependency management for Zig\n├── docs/                # Documentation and generated files\n├── src/                 # Source code\n│   ├── autograd.zig     # Automatic differentiation\n│   ├── data.zig         # Data loading and preprocessing\n│   ├── dtypes.zig       # Data type definitions\n│   ├── errors.zig       # Custom error handling\n│   ├── functional.zig   # Functional programming utilities\n│   ├── logger.zig       # Logging utilities\n│   ├── main.zig         # Entry point for the application\n│   ├── ndarray.zig      # Core tensor operations\n│   ├── nn.zig           # Neural network components\n│   ├── ops.zig          # Tensor operations\n│   ├── optim.zig        # Optimization algorithms\n│   ├── root.zig         # Root module for the library\n│   ├── tensor.zig       # Tensor abstraction\n│   ├── utils.zig        # Utility functions\n│   └── zorch.zig        # Main library module\n└── zig-out/             # Build output directory\n    ├── bin/             # Compiled binaries\n    │   └── zorch        # Executable\n    └── lib/             # Compiled libraries\n        └── libzorch.a   # Static library\n```\n\n## Getting Started\n\n### Prerequisites\n\n- [Zig](https://ziglang.org/download/) (version 0.13.0 or later)\n\n### Building the Project\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/your-username/zorch.git\n   cd zorch\n   ```\n\n2. Build the project using Zig:\n   ```bash\n   zig build\n   ```\n\n   This will generate the following outputs:\n   - Executable: `zig-out/bin/zorch`\n   - Static library: `zig-out/lib/libzorch.a`\n\n3. Run the executable:\n   ```bash\n   ./zig-out/bin/zorch\n   ```\n   Or use \n    ```bash\n   zig build run\n   ```\n\n### Using the Library\n\nTo use Zorch in your Zig project, add it as a dependency in your `build.zig.zon` file:\n\n```zig\n.dependencies = .{\n    .zorch = .{\n        .url = \"https://github.com/your-username/zorch/archive/main.tar.gz\",\n        .hash = \"your-hash-here\",\n    },\n},\n```\n\nThen, import the library in your Zig code:\n\n```zig\nconst zorch = @import(\"zorch\");\n\npub fn main() !void {\n    // Example usage\n    const allocator = std.heap.page_allocator;\n    const tensor = try zorch.Tensor.from_value(allocator, &[_]usize{2, 2}, .f32, 7.2);\n    defer tensor.deinit();\n\n    try tensor.print();\n}\n```\n\n## Documentation\n\nFor detailed documentation, refer to the [docs](./docs/index.html) directory. You can also generate the documentation locally:\n\n```bash\nzig build docs\n```\n\nThis will generate HTML documentation in the `docs/` directory.\n\n## Examples\n\n### Creating a Tensor\n\n```zig\nconst allocator = std.heap.page_allocator;\nconst tensor = try zorch.Tensor.from_value(allocator, &[_]usize{2, 2}, .f32, 1.0);\ndefer tensor.deinit();\n```\n\n### Performing Tensor Operations\n\n```zig\nconst a = try zorch.Tensor.from_value(allocator, &[_]usize{2, 2}, .f32, 1.0);\nconst b = try zorch.Tensor.from_value(allocator, &[_]usize{2, 2}, .f32, 2.0);\ndefer a.deinit();\ndefer b.deinit();\n\nconst result = try a.add(b, false);\ndefer result.deinit();\n```\n\n### Using Automatic Differentiation\n\n```zig\nconst x = try zorch.Tensor.from_value(allocator, &[_]usize{2, 2}, .f32, 1.0);\nx.requires_grad = true;\ndefer x.deinit();\n\nconst y = try x.mul_scalar(2.0, false);\ndefer y.deinit();\n\ntry y.backward(null);\n```\n\n## Contributing\n\n- Contributions are welcome! Please open an issue or submit a pull request for any bugs, feature requests, or improvements.\n- Let me know if you need further help!\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/15731839?u=46cfa76f9abf11263e4b2f2cfb86e6506e689ca8&v=4",
  "releases": [],
  "owner_bio": "MSc. in EE & IT from TUM,  ML engineer, programming enthusiast and coffee addict.\r\n",
  "owner_company": "@Huawei",
  "owner_location": "Munich",
  "owner_blog": "https://superkogito.github.io/",
  "owner_twitter_username": null,
  "owner_followers": 138,
  "owner_following": 129,
  "owner_created_at": "2015-11-09T15:04:54Z",
  "homepage": "https://superkogito.github.io/Zorch/",
  "license": "LGPL-3.0",
  "category": "library"
}