{
  "name": "bench",
  "owner": "pyk",
  "repo": "bench",
  "description": "Tiny benchmarking library for Zig",
  "type": "package",
  "topics": [
    "benchmark",
    "zig",
    "zig-package"
  ],
  "stars": 4,
  "forks": 0,
  "watchers": 0,
  "updated_at": "2025-12-07T07:58:57Z",
  "minimum_zig_version": "0.16.0-dev.1484+d0ba6642b",
  "readme": "# bench\n\nTiny benchmarking library for Zig.\n\n## Features\n\n- **CPU Counters**: Measures CPU cycles, instructions, IPC, and cache misses\n  directly from the kernel (Linux only).\n- **Argument Support**: Pass pre-calculated data to your functions to separate\n  setup overhead from the benchmark loop.\n- **Baseline Comparison**: Easily compare multiple implementations against a\n  reference function to see relative speedups or regressions.\n- **Flexible Reporting**: Access raw metric data programmatically to generate\n  custom reports (JSON, CSV) or assert performance limits in CI.\n- **Easy Throughput Metrics**: Automatically calculates operations per second\n  and data throughput (MB/s, GB/s) when payload size is provided.\n- **Robust Statistics**: Uses median and standard deviation to provide reliable\n  metrics despite system noise.\n- **Zero Dependencies**: Implemented in pure Zig using only the standard\n  library.\n\n## Installation\n\nFetch latest version:\n\n```sh\nzig fetch --save=bench https://github.com/pyk/bench/archive/main.tar.gz\n```\n\nAdd `bench` as a dependency to your `build.zig`.\n\nIf you are using it only for tests/benchmarks, it is recommended to mark it as\nlazy:\n\n```zig\n.dependencies = .{\n    .bench = .{\n        .url = \"...\",\n        .hash = \"...\",\n        .lazy = true, // here\n    },\n}\n```\n\n## Usage\n\n### Basic Run\n\nTo benchmark a single function, pass the allocator, a name, and the function\npointer to `run`.\n\n```zig\nconst res = try bench.run(allocator, \"My Function\", myFn, .{});\ntry bench.report({ .metrics = &.{res} });\n```\n\n### Run with Arguments\n\nYou can generate test data before the benchmark starts and pass it via a tuple.\nThis ensures the setup cost doesn't pollute your measurements.\n\n```zig\n// Setup data outside the benchmark\nconst input = try generateLargeString(allocator, 10_000);\n\n// Pass input as a tuple\nconst res = try bench.run(allocator, \"Parser\", parseFn, .{input}, .{});\n```\n\n### Comparing Implementations\n\nYou can run multiple benchmarks and compare them against a baseline. The\n`baseline_index` determines which result is used as the reference (1.00x).\n\n```zig\nconst a = try bench.run(allocator, \"Implementation A\", implA, .{});\nconst b = try bench.run(allocator, \"Implementation B\", implB, .{});\n\ntry bench.report(.{\n    .metrics = &.{ a, b },\n    // Use the first metric (Implementation A) as the baseline\n    .baseline_index = 0,\n});\n```\n\n### Measuring Throughput\n\nIf your function processes data (like copying memory or parsing strings),\nprovide `bytes_per_op` to get throughput metrics (MB/s or GB/s).\n\n```zig\nconst size = 1024 * 1024;\nconst res = try bench.run(allocator, \"Memcpy 1MB\", copyFn, .{\n    .bytes_per_op = size,\n});\n\n// Report will now show GB/s instead of just Ops/s\ntry bench.report({ .metrics = &.{res} });\n```\n\n### Configuration\n\nYou can tune the benchmark behavior by modifying the `Options` struct.\n\n```zig\nconst res = try bench.run(allocator, \"Heavy Task\", heavyFn, .{\n    .warmup_iters = 10,     // Default: 100\n    .sample_size = 50,      // Default: 1000\n});\n```\n\n### Built-in Reporter\n\nThe default `bench.report` prints a human-readable table to stdout. It handles\nunits (ns, us, ms, s) and coloring automatically.\n\n```sh\nBenchmark Summary: 3 benchmarks run\n├─ NoOp        60ns      16.80M/s   [baseline]\n│  └─ cycles: 14        instructions: 36        ipc: 2.51       miss: 0\n├─ Sleep     1.06ms         944/s   17648.20x slower\n│  └─ cycles: 4.1k      instructions: 2.9k      ipc: 0.72       miss: 17\n└─ Busy     32.38us      30.78K/s   539.68x slower\n   └─ cycles: 150.1k    instructions: 700.1k    ipc: 4.67       miss: 0\n```\n\n### Custom Reporter\n\nThe `run` function returns a `Metrics` struct containing all raw data (min, max,\nmedian, variance, cycles, etc.). You can use this to generate JSON, CSV, or\nassert performance limits in CI.\n\n```zig\nconst metrics = try bench.run(allocator, \"MyFn\", myFn, .{});\n\n// Access raw fields directly\nstd.debug.print(\"Median: {d}ns, Max: {d}ns\\n\", .{\n    metrics.median_ns,\n    metrics.max_ns\n});\n```\n\n## Supported Metrics\n\nThis tiny benchmark library support (✅) various metrics:\n\n| Category    | Metric                       | Description                                                  |\n| ----------- | ---------------------------- | ------------------------------------------------------------ |\n| Time        | ✅ Mean / Average            | Arithmetic average of all runs                               |\n| Time        | ✅ Median                    | The middle value (less sensitive to outliers)                |\n| Time        | ✅ Min / Max                 | The absolute fastest and slowest runs                        |\n| Time        | CPU vs Wall Time             | CPU time (active processing) vs Wall time (real world)       |\n| Throughput  | ✅ Ops/sec                   | Operations per second                                        |\n| Throughput  | ✅ Bytes/sec                 | Data throughput (MB/s, GB/s)                                 |\n| Throughput  | Items/sec                    | Discrete items processed per second                          |\n| Latency     | Percentiles                  | p75, p99, p99.9. (e.g. \"99% of requests were faster than X\") |\n| Latency     | ✅ Std Dev / Variance        | How much the results deviate from the average                |\n| Latency     | Outliers                     | Detecting and reporting anomaly runs                         |\n| Latency     | Confidence / Margin of Error | e.g. \"± 2.5%\"                                                |\n| Latency     | Histogram                    | Visual distribution of all runs                              |\n| Memory      | Bytes Allocated              | Total heap memory requested per iteration                    |\n| Memory      | Allocation Count             | Number of allocation calls                                   |\n| CPU         | ✅ Cycles                    | CPU clock cycles used                                        |\n| CPU         | ✅ Instructions              | Total CPU instructions executed                              |\n| CPU         | ✅ IPC                       | Instructions Per Cycle (Efficiency)                          |\n| CPU         | ✅ Cache Misses              | L1/L2 Cache misses                                           |\n| Comparative | ✅ Speedup (x)               | \"12.5x faster\" (Current / Baseline).                         |\n| Comparative | Relative Diff (%)            | \"+ 50%\" or \"- 10%\".                                          |\n| Comparative | Big O                        | Complexity Analysis (O(n), O(log n)).                        |\n| Comparative | R² (Goodness of Fit)         | How well the data fits a linear model.                       |\n\nOther metrics will be added as needed. Feel free to send a pull request.\n\n## Notes\n\n- This library is designed to show you \"what\", not \"why\". I recommend using a\n  proper profiling tool such as `perf` on linux + Firefox Profiler to answer\n  \"why\".\n- `doNotOptimizeAway` is your friend. For example if you are benchmarking some\n  scanner/tokenizer:\n\n  ```zig\n    while (true) {\n        const token = try scanner.next();\n        if (token == .end) break;\n        total_ops += 1;\n        std.mem.doNotOptimizeAway(token); // CRITICAL\n    }\n  ```\n\n- To get `cycles`, `instructions`, `ipc` (instructions per cycle) and\n  `cache_misses` metrics on Linux, you may need to enable the\n  `kernel.perf_event_paranoid`.\n\n## Prior Art\n\n- [hendriknielaender/zBench](https://github.com/hendriknielaender/zBench)\n- [Hejsil/zig-bench](https://github.com/Hejsil/zig-bench)\n- [briangold/metron](https://github.com/briangold/metron)\n- [dweiller/zubench](https://github.com/dweiller/zubench)\n- [briangold/metron](https://github.com/briangold/metron)\n\n## Development\n\nInstall the Zig toolchain via mise (optional):\n\n```shell\nmise trust\nmise install\n```\n\nRun tests:\n\n```bash\nzig build test --summary all\n```\n\nBuild library:\n\n```bash\nzig build\n```\n\nEnable/disable `kernel.perf_event_paranoid` for debugging:\n\n```sh\n# Disable\nsudo sysctl -w kernel.perf_event_paranoid=3\n\n# Enable\nsudo sysctl -w kernel.perf_event_paranoid=-1\n```\n\n## License\n\nMIT. Use it for whatever.\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/2213646?u=a3fbef318b9ef3be210d4515e59455bd03b72a69&v=4",
  "releases": [],
  "owner_company": null,
  "owner_location": null,
  "owner_blog": "https://pyk.sh",
  "owner_twitter_username": "sepyke",
  "owner_followers": 366,
  "owner_following": 77,
  "owner_created_at": "2012-08-24T19:47:26Z",
  "license": "MIT",
  "category": "library"
}