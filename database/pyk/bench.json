{
  "name": "bench",
  "owner": "pyk",
  "repo": "bench",
  "description": "Fast & Accurate Benchmarking for Zig",
  "type": "package",
  "topics": [
    "benchmark",
    "zig",
    "zig-package"
  ],
  "stars": 4,
  "forks": 0,
  "watchers": 0,
  "updated_at": "2025-12-08T11:03:36Z",
  "minimum_zig_version": "0.16.0-dev.1484+d0ba6642b",
  "readme": "<p align=\"center\">\n  <a href=\"https://pyk.sh\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/pyk/bench/blob/main/.github/logo-dark.svg\">\n      <img alt=\"pyk/bench logo\" src=\"https://github.com/pyk/bench/blob/main/.github/logo-light.svg\">\n    </picture>\n  </a>\n</p>\n\n<p align=\"center\">\n  Fast & Accurate Benchmarking for Zig\n<p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/zig-0.16.0--dev-x?style=flat&labelColor=00f&color=fff&style=flat&logo=zig&logoColor=fff\" alt=\"Zig Version\">\n  <img src=\"https://img.shields.io/badge/version-alpha-x?style=flat&labelColor=00f&color=fff&style=flat\" alt=\"Alpha lib\">\n  <img src=\"https://img.shields.io/github/check-runs/pyk/bench/main?colorA=00f&colorB=fff&style=flat&logo=github\" alt=\"CI Runs\">\n  <img src=\"https://img.shields.io/github/license/pyk/bench?colorA=00f&colorB=fff&style=flat\" alt=\"MIT License\">\n</p>\n\n## Features\n\n- **CPU Counters**: Measures CPU cycles, instructions, IPC, and cache misses\n  directly from the kernel (Linux only).\n- **Argument Support**: Pass pre-calculated data to your functions to separate\n  setup overhead from the benchmark loop.\n- **Baseline Comparison**: Easily compare multiple implementations against a\n  reference function to see relative speedups or regressions.\n- **Flexible Reporting**: Access raw metric data programmatically to generate\n  custom reports (JSON, CSV) or assert performance limits in CI.\n- **Easy Throughput Metrics**: Automatically calculates operations per second\n  and data throughput (MB/s, GB/s) when payload size is provided.\n- **Robust Statistics**: Uses median and standard deviation to provide reliable\n  metrics despite system noise.\n- **Zero Dependencies**: Implemented in pure Zig using only the standard\n  library.\n\n## Installation\n\nFetch latest version:\n\n```sh\nzig fetch --save=bench https://github.com/pyk/bench/archive/main.tar.gz\n```\n\nThen add this to your `build.zig`:\n\n```zig\nconst bench = b.dependency(\"bench\", .{\n    .target = target,\n    .optimize = optimize,\n});\n\n// Use it on a module\nconst mod = b.createModule(.{\n    .target = target,\n    .optimize = optimize,\n    .imports = &.{\n        .{ .name = \"bench\", .module = bench.module(\"bench\") },\n    },\n});\n\n// Or executable\nconst my_bench = b.addExecutable(.{\n    .name = \"my-bench\",\n    .root_module = b.createModule(.{\n        .root_source_file = b.path(\"bench/my-bench.zig\"),\n        .target = target,\n        .optimize = .ReleaseFast,\n        .imports = &.{\n            .{ .name = \"bench\", .module = bench.module(\"bench\") },\n        },\n    }),\n});\n```\n\nIf you are using it only for tests/benchmarks, it is recommended to mark it as\nlazy:\n\n```zig\n.dependencies = .{\n    .bench = .{\n        .url = \"...\",\n        .hash = \"...\",\n        .lazy = true, // here\n    },\n}\n```\n\n## Usage\n\n### Basic Run\n\nTo benchmark a single function, pass the allocator, a name, and the function\npointer to `run`.\n\n```zig\nconst res = try bench.run(allocator, \"My Function\", myFn, .{});\ntry bench.report(.{ .metrics = &.{res} });\n```\n\n### Run with Arguments\n\nYou can generate test data before the benchmark starts and pass it via a tuple.\nThis ensures the setup cost doesn't pollute your measurements.\n\n```zig\n// Setup data outside the benchmark\nconst input = try generateLargeString(allocator, 10_000);\n\n// Pass input as a tuple\nconst res = try bench.run(allocator, \"Parser\", parseFn, .{input}, .{});\n```\n\n### Comparing Implementations\n\nYou can run multiple benchmarks and compare them against a baseline. The\n`baseline_index` determines which result is used as the reference (1.00x).\n\n```zig\nconst a = try bench.run(allocator, \"Implementation A\", implA, .{});\nconst b = try bench.run(allocator, \"Implementation B\", implB, .{});\n\ntry bench.report(.{\n    .metrics = &.{ a, b },\n    // Use the first metric (Implementation A) as the baseline\n    .baseline_index = 0,\n});\n```\n\n### Measuring Throughput\n\nIf your function processes data (like copying memory or parsing strings),\nprovide `bytes_per_op` to get throughput metrics (MB/s or GB/s).\n\n```zig\nconst size = 1024 * 1024;\nconst res = try bench.run(allocator, \"Memcpy 1MB\", copyFn, .{\n    .bytes_per_op = size,\n});\n\n// Report will now show GB/s instead of just Ops/s\ntry bench.report({ .metrics = &.{res} });\n```\n\n### Configuration\n\nYou can tune the benchmark behavior by modifying the `Options` struct.\n\n```zig\nconst res = try bench.run(allocator, \"Heavy Task\", heavyFn, .{\n    .warmup_iters = 10,     // Default: 100\n    .sample_size = 50,      // Default: 1000\n});\n```\n\n### Built-in Reporter\n\nThe default `bench.report` prints a human-readable table to stdout. It handles\nunits (ns, us, ms, s) and coloring automatically.\n\n```sh\n$ zig build quicksort\nBenchmarking Sorting Algorithms Against Random Input (N=10000)...\nBenchmark Summary: 3 benchmarks run\n├─ Unsafe Quicksort (Lomuto)   358.64us    110.98MB/s   1.29x faster\n│  └─ cycles: 1.6M      instructions: 1.2M      ipc: 0.75       miss: 65\n├─ Unsafe Quicksort (Hoare)    383.02us    104.32MB/s   1.21x faster\n│  └─ cycles: 1.7M      instructions: 1.3M      ipc: 0.76       miss: 56\n└─ std.mem.sort                462.25us     86.45MB/s   [baseline]\n   └─ cycles: 2.0M      instructions: 2.6M      ipc: 1.30       miss: 143\n```\n\n### Custom Reporter\n\nThe `run` function returns a `Metrics` struct containing all raw data (min, max,\nmedian, variance, cycles, etc.). You can use this to generate JSON, CSV, or\nassert performance limits in CI.\n\n```zig\nconst metrics = try bench.run(allocator, \"MyFn\", myFn, .{});\n\n// Access raw fields directly\nstd.debug.print(\"Median: {d}ns, Max: {d}ns\\n\", .{\n    metrics.median_ns,\n    metrics.max_ns\n});\n```\n\n## Supported Metrics\n\nThe `run` function returns a `Metrics` struct containing the following data\npoints:\n\n| Category   | Metric         | Description                                                |\n| ---------- | -------------- | ---------------------------------------------------------- |\n| Meta       | `name`         | The identifier string for the benchmark.                   |\n| Time       | `min_ns`       | Minimum execution time per operation (nanoseconds).        |\n| Time       | `max_ns`       | Maximum execution time per operation (nanoseconds).        |\n| Time       | `mean_ns`      | Arithmetic mean execution time (nanoseconds).              |\n| Time       | `median_ns`    | Median execution time (nanoseconds).                       |\n| Time       | `std_dev_ns`   | Standard deviation of the execution time.                  |\n| Meta       | `samples`      | Total number of measurement samples collected.             |\n| Throughput | `ops_sec`      | Calculated operations per second.                          |\n| Throughput | `mb_sec`       | Data throughput in MB/s (populated if `bytes_per_op` > 0). |\n| Hardware\\* | `cycles`       | Average CPU cycles per operation.                          |\n| Hardware\\* | `instructions` | Average CPU instructions executed per operation.           |\n| Hardware\\* | `ipc`          | Instructions Per Cycle (efficiency ratio).                 |\n| Hardware\\* | `cache_misses` | Average cache misses per operation.                        |\n\n_\\*Hardware metrics are currently available on Linux only. They will be `null`\non other platforms or if permissions are restricted._\n\n## Notes\n\n- This library is designed to show you \"what\", not \"why\". I recommend using a\n  proper profiling tool such as `perf` on linux + Firefox Profiler to answer\n  \"why\".\n- `doNotOptimizeAway` is your friend. For example if you are benchmarking some\n  scanner/tokenizer:\n\n  ```zig\n    while (true) {\n        const token = try scanner.next();\n        if (token == .end) break;\n        total_ops += 1;\n        std.mem.doNotOptimizeAway(token); // CRITICAL\n    }\n  ```\n\n- To get `cycles`, `instructions`, `ipc` (instructions per cycle) and\n  `cache_misses` metrics on Linux, you may need to enable the\n  `kernel.perf_event_paranoid`.\n\n## Prior Art\n\n- [hendriknielaender/zBench](https://github.com/hendriknielaender/zBench)\n- [Hejsil/zig-bench](https://github.com/Hejsil/zig-bench)\n- [briangold/metron](https://github.com/briangold/metron)\n- [dweiller/zubench](https://github.com/dweiller/zubench)\n\n## Development\n\nInstall the Zig toolchain via mise (optional):\n\n```shell\nmise trust\nmise install\n```\n\nRun tests:\n\n```bash\nzig build test --summary all\n```\n\nBuild library:\n\n```bash\nzig build\n```\n\nEnable/disable `kernel.perf_event_paranoid` for debugging:\n\n```sh\n# Disable\nsudo sysctl -w kernel.perf_event_paranoid=2\n\n# Enable\nsudo sysctl -w kernel.perf_event_paranoid=-1\n```\n\n## Devlog\n\n- [Fixing Microbenchmark Accuracy](https://pyk.sh/blog/2025-12-07-bench-fixing-microbenchmark-accuracy-in-zig)\n- [Fixing Zig benchmark where `std.mem.doNotOptimizeAway` was ignored](https://pyk.sh/blog/2025-12-08-bench-fixing-constant-folding)\n\n## License\n\nMIT. Use it for whatever.\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/2213646?u=a3fbef318b9ef3be210d4515e59455bd03b72a69&v=4",
  "releases": [],
  "owner_company": null,
  "owner_location": null,
  "owner_blog": "https://pyk.sh",
  "owner_twitter_username": "sepyke",
  "owner_followers": 366,
  "owner_following": 77,
  "owner_created_at": "2012-08-24T19:47:26Z",
  "license": "MIT",
  "category": "library"
}