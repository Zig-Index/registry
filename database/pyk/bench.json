{
  "name": "bench",
  "owner": "pyk",
  "repo": "bench",
  "description": "Fast & Accurate Microbenchmarking for Zig",
  "type": "package",
  "topics": [
    "benchmark",
    "zig",
    "zig-package",
    "microbenchmark"
  ],
  "stars": 17,
  "forks": 0,
  "watchers": 0,
  "updated_at": "2026-01-04T07:04:40Z",
  "minimum_zig_version": "0.16.0-dev.1484+d0ba6642b",
  "readme": "<p align=\"center\">\n  <a href=\"https://pyk.sh\">\n    <picture>\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://github.com/pyk/bench/blob/main/.github/logo-dark.svg\">\n      <img alt=\"pyk/bench logo\" src=\"https://github.com/pyk/bench/blob/main/.github/logo-light.svg\">\n    </picture>\n  </a>\n</p>\n\n<p align=\"center\">\n  Fast & Accurate Microbenchmarking for Zig\n<p>\n\n<p align=\"center\">\n  <img src=\"https://img.shields.io/badge/zig-0.16.0--dev-x?style=flat&labelColor=00f&color=fff&style=flat&logo=zig&logoColor=fff\" alt=\"Zig Version\">\n  <img src=\"https://img.shields.io/badge/version-alpha-x?style=flat&labelColor=00f&color=fff&style=flat\" alt=\"Alpha lib\">\n  <img src=\"https://img.shields.io/github/check-runs/pyk/bench/main?colorA=00f&colorB=fff&style=flat&logo=github\" alt=\"CI Runs\">\n  <img src=\"https://img.shields.io/github/license/pyk/bench?colorA=00f&colorB=fff&style=flat\" alt=\"MIT License\">\n</p>\n\n## Demo\n\nLet's benchmark fib:\n\n```zig\nconst std = @import(\"std\");\nconst bench = @import(\"bench\");\n\nfn fibNaive(n: u64) u64 {\n    if (n <= 1) return n;\n    return fibNaive(n - 1) + fibNaive(n - 2);\n}\n\nfn fibIterative(n: u64) u64 {\n    if (n == 0) return 0;\n    var a: u64 = 0;\n    var b: u64 = 1;\n    for (2..n + 1) |_| {\n        const c = a + b;\n        a = b;\n        b = c;\n    }\n    return b;\n}\n\npub fn main() !void {\n    const allocator = std.heap.smp_allocator;\n    const opts = bench.Options{\n        .sample_size = 100,\n        .warmup_iters = 3,\n    };\n    const m_naive = try bench.run(allocator, \"fibNaive/30\", fibNaive, .{30}, opts);\n    const m_iter = try bench.run(allocator, \"fibIterative/30\", fibIterative, .{30}, opts);\n\n    try bench.report(.{\n        .metrics = &.{ m_naive, m_iter },\n        .baseline_index = 0, // naive as baseline\n    });\n}\n```\n\nRun it, and you will get the following output in your terminal:\n\n```markdown\n| Benchmark         |    Time |    Speedup | Iterations |    Ops/s | Cycles | Instructions |  IPC | Cache Misses |\n| :---------------- | ------: | ---------: | ---------: | -------: | -----: | -----------: | ---: | -----------: |\n| `fibNaive/30`     | 1.78 ms |      1.00x |          1 |  563.2/s |   8.1M |        27.8M | 3.41 |          0.3 |\n| `fibIterative/30` | 3.44 ns | 516055.19x |     300006 | 290.6M/s |   15.9 |         82.0 | 5.15 |          0.0 |\n```\n\nThe benchmark report generates valid Markdown, so you can copy-paste it directly\ninto a markdown file:\n\n| Benchmark         |    Time |    Speedup | Iterations |    Ops/s | Cycles | Instructions |  IPC | Cache Misses |\n| :---------------- | ------: | ---------: | ---------: | -------: | -----: | -----------: | ---: | -----------: |\n| `fibNaive/30`     | 1.78 ms |      1.00x |          1 |  563.2/s |   8.1M |        27.8M | 3.41 |          0.3 |\n| `fibIterative/30` | 3.44 ns | 516055.19x |     300006 | 290.6M/s |   15.9 |         82.0 | 5.15 |          0.0 |\n\n## Features\n\n- **CPU Counters**: Measures CPU cycles, instructions, IPC, and cache misses\n  directly from the kernel (Linux only).\n- **Argument Support**: Pass pre-calculated data to your functions to separate\n  setup overhead from the benchmark loop.\n- **Baseline Comparison**: Easily compare multiple implementations against a\n  reference function to see relative speedups or regressions.\n- **Flexible Reporting**: Access raw metric data programmatically to generate\n  custom reports (JSON, CSV) or assert performance limits in CI.\n- **Easy Throughput Metrics**: Automatically calculates operations per second\n  and data throughput (MB/s, GB/s) when payload size is provided.\n- **Robust Statistics**: Uses median and standard deviation to provide reliable\n  metrics despite system noise.\n\n## Installation\n\nFetch the latest version:\n\n```sh\nzig fetch --save=bench https://github.com/pyk/bench/archive/main.tar.gz\n```\n\nThen add this to your `build.zig`:\n\n```zig\nconst bench = b.dependency(\"bench\", .{\n    .target = target,\n    .optimize = optimize,\n});\n\n// Use it on a module\nconst mod = b.createModule(.{\n    .target = target,\n    .optimize = optimize,\n    .imports = &.{\n        .{ .name = \"bench\", .module = bench.module(\"bench\") },\n    },\n});\n\n// Or executable\nconst my_bench = b.addExecutable(.{\n    .name = \"my-bench\",\n    .root_module = b.createModule(.{\n        .root_source_file = b.path(\"bench/my-bench.zig\"),\n        .target = target,\n        .optimize = .ReleaseFast,\n        .imports = &.{\n            .{ .name = \"bench\", .module = bench.module(\"bench\") },\n        },\n    }),\n});\n```\n\nIf you are using it only for tests/benchmarks, it is recommended to mark it as\nlazy:\n\n```zig\n.dependencies = .{\n    .bench = .{\n        .url = \"...\",\n        .hash = \"...\",\n        .lazy = true, // here\n    },\n}\n```\n\n## Usage\n\n### Basic Run\n\nTo benchmark a single function, pass the allocator, a name, and the function\npointer to `run`.\n\n```zig\nconst res = try bench.run(allocator, \"My Function\", myFn, .{});\ntry bench.report(.{ .metrics = &.{res} });\n```\n\n### Run with Arguments\n\nYou can generate test data before the benchmark starts and pass it via a tuple.\nThis ensures the setup cost doesn't pollute your measurements.\n\n```zig\n// Setup data outside the benchmark\nconst input = try generateLargeString(allocator, 10_000);\n\n// Pass input as a tuple\nconst res = try bench.run(allocator, \"Parser\", parseFn, .{input}, .{});\n```\n\n### Comparing Implementations\n\nYou can run multiple benchmarks and compare them against a baseline. The\n`baseline_index` determines which result is used as the reference (1.00x).\n\n```zig\nconst a = try bench.run(allocator, \"Implementation A\", implA, .{});\nconst b = try bench.run(allocator, \"Implementation B\", implB, .{});\n\ntry bench.report(.{\n    .metrics = &.{ a, b },\n    // Use the first metric (Implementation A) as the baseline\n    .baseline_index = 0,\n});\n```\n\n### Measuring Throughput\n\nIf your function processes data (like copying memory or parsing strings),\nprovide `bytes_per_op` to get throughput metrics (MB/s or GB/s).\n\n```zig\nconst size = 1024 * 1024;\nconst res = try bench.run(allocator, \"Memcpy 1MB\", copyFn, .{\n    .bytes_per_op = size,\n});\n\n// Report will now show GB/s instead of just Ops/s\ntry bench.report(.{ .metrics = &.{res} });\n```\n\n### Configuration\n\nYou can tune the benchmark behavior by modifying the `Options` struct.\n\n```zig\nconst res = try bench.run(allocator, \"Heavy Task\", heavyFn, .{\n    .warmup_iters = 10,     // Default: 100\n    .sample_size = 50,      // Default: 1000\n});\n```\n\n### Built-in Reporter\n\nThe default bench.report prints a clean, Markdown-compatible table to stdout. It\nautomatically handles unit scaling (`ns`, `us`, `ms`, `s`) and formatting.\n\n```markdown\n| Benchmark         |    Time |    Speedup | Iterations |    Ops/s | Cycles | Instructions |  IPC | Cache Misses |\n| :---------------- | ------: | ---------: | ---------: | -------: | -----: | -----------: | ---: | -----------: |\n| `fibNaive/30`     | 1.78 ms |      1.00x |          1 |  563.2/s |   8.1M |        27.8M | 3.41 |          0.3 |\n| `fibIterative/30` | 3.44 ns | 516055.19x |     300006 | 290.6M/s |   15.9 |         82.0 | 5.15 |          0.0 |\n```\n\n### Custom Reporter\n\nThe `run` function returns a `Metrics` struct containing all raw data (min, max,\nmedian, variance, cycles, etc.). You can use this to generate JSON, CSV, or\nassert performance limits in CI.\n\n```zig\nconst metrics = try bench.run(allocator, \"MyFn\", myFn, .{});\n\n// Access raw fields directly\nstd.debug.print(\"Median: {d}ns, Max: {d}ns\\n\", .{\n    metrics.median_ns,\n    metrics.max_ns\n});\n```\n\n## Supported Metrics\n\nThe `run` function returns a `Metrics` struct containing the following data\npoints:\n\n| Category   | Metric         | Description                                                |\n| ---------- | -------------- | ---------------------------------------------------------- |\n| Meta       | `name`         | The identifier string for the benchmark.                   |\n| Time       | `min_ns`       | Minimum execution time per operation (nanoseconds).        |\n| Time       | `max_ns`       | Maximum execution time per operation (nanoseconds).        |\n| Time       | `mean_ns`      | Arithmetic mean execution time (nanoseconds).              |\n| Time       | `median_ns`    | Median execution time (nanoseconds).                       |\n| Time       | `std_dev_ns`   | Standard deviation of the execution time.                  |\n| Meta       | `samples`      | Total number of measurement samples collected.             |\n| Throughput | `ops_sec`      | Calculated operations per second.                          |\n| Throughput | `mb_sec`       | Data throughput in MB/s (populated if `bytes_per_op` > 0). |\n| Hardware\\* | `cycles`       | Average CPU cycles per operation.                          |\n| Hardware\\* | `instructions` | Average CPU instructions executed per operation.           |\n| Hardware\\* | `ipc`          | Instructions Per Cycle (efficiency ratio).                 |\n| Hardware\\* | `cache_misses` | Average cache misses per operation.                        |\n\n_\\*Hardware metrics are currently available on Linux only. They will be `null`\non other platforms or if permissions are restricted._\n\n## Tips\n\n### Use Profiling Tools to Find Why Code is Slow\n\n`bench` shows you the time your code takes. It tells you \"what\" the speed is.\nBut it does not tell you \"why\" it is slow. To find out why, use tools like\n`perf` on Linux. These tools show you where the CPU spends time. For example,\n`perf record` runs your code and collects data. Then `perf report` or\n[Firefox Profiler](https://profiler.firefox.com/) shows hotspots. This helps you\nfix the real problems.\n\n### Use `std.mem.doNotOptimizeAway`\n\nThe compiler can remove code if it thinks it does nothing. For example, if you\ncompute a value but never use it, the compiler skips the work. This makes\nbenchmarks wrong. It shows fast times for code that does not run.\n\nTo stop this, use `std.mem.doNotOptimizeAway`. Pass your result to it. The\ncompiler must compute it then. For example, in a scanner or tokenizer:\n\n```zig\nwhile (true) {\n    const token = try scanner.next();\n    if (token == .end) break;\n    std.mem.doNotOptimizeAway(token); // CRITICAL\n}\n```\n\nHere, `doNotOptimizeAway(token)` forces the compiler to run `scanner.next()`.\nWithout it, the loop might empty. Always use this on key results. Like counts,\nparsed values, or outputs.\n\n### Enable Kernel Perf Event on Linux for Hardware Metrics\n\nOn Linux, hardware metrics like cycles and instructions come from the kernel.\nBut by default, it limits access. You get null values.\n\nTo fix, run:\n\n```sh\nsudo sysctl -w kernel.perf_event_paranoid=-1\n```\n\nThis allows your code to read counters. Set to `2` to restrict again.\n\nCheck with `cat /proc/sys/kernel/perf_event_paranoid`. Lower values mean more\naccess. Value `-1` is full. Use it for benchmarks. But be careful in production.\n\n### Avoid Constant Inputs in Benchmarks\n\nIf you use constant data like `const input = \"hello\";`, the compiler knows it at\nbuild time. It can unroll loops or compute results ahead. Your benchmark\nmeasures nothing real. Times stay flat even if data grows.\n\nInstead, use runtime data. Allocate a buffer and fill it.\n\nBad example:\n\n```zig\nconst input = \"    hello\"; // Compiler knows every byte\nconst res = try bench.run(allocator, \"Parser\", parse, .{input}, .{});\n```\n\nGood example:\n\n```zig\nvar input = try allocator.alloc(u8, 100);\ndefer allocator.free(input);\n@memset(input[0..4], ' ');\n@memcpy(input[4..], \"hello\");\nconst res = try bench.run(allocator, \"Parser\", parse, .{input}, .{});\n```\n\nNow, the buffer is dynamic. The compiler cannot fold it. Times scale with real\nwork. For varying tests, change the memset size each run.\n\n## References\n\n### Devlog\n\n- [Fixing Microbenchmark Accuracy](https://pyk.sh/blog/2025-12-07-bench-fixing-microbenchmark-accuracy-in-zig)\n- [Fixing Zig benchmark where `std.mem.doNotOptimizeAway` was ignored](https://pyk.sh/blog/2025-12-08-bench-fixing-constant-folding)\n- [Writing a Type-Safe Linux Perf Interface in Zig](https://pyk.sh/blog/2025-12-11-type-safe-linux-perf-event-open-in-zig)\n\n### Prior Art\n\n- [hendriknielaender/zBench](https://github.com/hendriknielaender/zBench)\n- [Hejsil/zig-bench](https://github.com/Hejsil/zig-bench)\n- [briangold/metron](https://github.com/briangold/metron)\n- [dweiller/zubench](https://github.com/dweiller/zubench)\n\n### Resources\n\n- [Google Benchmark User Guide](https://github.com/google/benchmark/blob/main/docs/user_guide.md)\n\n## Development\n\nInstall the Zig toolchain via mise (optional):\n\n```shell\nmise trust\nmise install\n```\n\nRun tests:\n\n```bash\nzig build test --summary all\n```\n\nBuild library:\n\n```bash\nzig build\n```\n\nEnable/disable `kernel.perf_event_paranoid` for debugging:\n\n```sh\n# Restrict access\nsudo sysctl -w kernel.perf_event_paranoid=2\n\n# Allow access (Required for CPU metrics)\nsudo sysctl -w kernel.perf_event_paranoid=-1\n```\n\n## License\n\nMIT. Use it for whatever.\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/2213646?u=a3fbef318b9ef3be210d4515e59455bd03b72a69&v=4",
  "releases": [],
  "owner_company": null,
  "owner_location": null,
  "owner_blog": "https://pyk.sh",
  "owner_twitter_username": "sepyke",
  "owner_followers": 372,
  "owner_following": 77,
  "owner_created_at": "2012-08-24T19:47:26Z",
  "license": "MIT",
  "category": "library"
}