{
  "name": "pg.zig",
  "owner": "karlseguin",
  "repo": "pg.zig",
  "description": "Native PostgreSQL driver / client for Zig",
  "type": "package",
  "topics": [
    "postgresql-client",
    "zig",
    "zig-library",
    "zig-package"
  ],
  "stars": 458,
  "forks": 40,
  "watchers": 3,
  "updated_at": "2025-12-05T19:18:26Z",
  "dependencies": [
    {
      "name": "buffer",
      "url": "https://github.com/karlseguin/buffer.zig/archive/30f9512f61127efda1049c9064e533efa82c26f7.tar.gz",
      "hash": "N-V-__8AAEGLAAB4JS8S1rWwdvXUTwnt7gRNthhJanWx4AvP"
    },
    {
      "name": "metrics",
      "url": "https://github.com/karlseguin/metrics.zig/archive/13d8706e1ae921a8cc0d2f88283c1b5412c73e2f.tar.gz",
      "hash": "metrics-0.0.0-W7G4eP2_AQBKsaql3dhLJ-pkf-RdP-zV3vflJy4N34jC"
    }
  ],
  "readme": "# Native PostgreSQL driver for Zig\n\nA native PostgresSQL driver / client for Zig. Supports [LISTEN](#listen--notify).\n\nSee or run [example/main.zig](https://github.com/karlseguin/pg.zig/blob/master/example/main.zig) for a number of examples.\n\n## Install\n1) Add pg.zig as a dependency in your `build.zig.zon`:\n\n```bash\nzig fetch --save git+https://github.com/karlseguin/pg.zig#master\n```\n\n2) In your `build.zig`, add the `pg` module as a dependency you your program:\n\n```zig\nconst pg = b.dependency(\"pg\", .{\n    .target = target,\n    .optimize = optimize,\n});\n\n// the executable from your call to b.addExecutable(...)\nexe.root_module.addImport(\"pg\", pg.module(\"pg\"));\n```\n\n## Example\n```zig\nvar pool = try pg.Pool.init(allocator, .{\n  .size = 5,\n  .connect = .{\n    .port = 5432,\n    .host = \"127.0.0.1\",\n  },\n  .auth = .{\n    .username = \"postgres\",\n    .database = \"postgres\",\n    .password = \"postgres\",\n    .timeout = 10_000,\n  }\n});\ndefer pool.deinit();\n\nvar result = try pool.query(\"select id, name from users where power > $1\", .{9000});\ndefer result.deinit();\n\nwhile (try result.next()) |row| {\n  const id = row.get(i32, 0);\n  // this is only valid until the next call to next(), deinit() or drain()\n  const name = row.get([]u8, 1);\n}\n```\n\n## Pool\nThe pool keeps a configured number of database connection open. The `acquire()` method is used to retrieve a connection from the pool. The pool may start one background thread to attempt to reconnect disconnected connections (or connections which are in an invalid state).\n\n### init(allocator: std.mem.allocator, opts: Opts) !*Pool\nInitializes a connection pool. Pool options are:\n\n* `size` - Number of connections to maintain. Defaults to `10`\n* `auth`: - See [Conn.auth](#authopts-opts-void)\n* `connect`: - See the [Conn.open](#openallocator-stdmemallocator-opts-opts-conn)\n* `timeout`: - The amount of time, in milliseconds, to wait for a connection to be available when `acquire()` is called.\n* `connect_on_init_count`:  - The # of connections in the pool to eagerly connect during `init`. Defaults to `null` which will initiliaze all connections (`size`). The background reconnector is used to setup the remaining (`size - connect_on_init_count`) connections. This can be set to `0`, to prevent `init` from failing except in extreme cases (i.e. OOM), but that will hide any configuration/connection issue until the first query is executed.\n\n### initUri(allocator: std.mem.Allocator, uri: std.Uri, opts: Opts) !*Pool\nInitializes a connection pool using a std.Uri. When using this function, the `auth` and `connect` fields of `opts` should **not** be set, as these will automatically set based on the provided `uri`.\n\n```zig\nconst uri = try std.Uri.parse(\"postgresql://username:password@localhost:5432/database_name\");\nconst pool = try pg.Pool.initUri(allocator, uri, 5, 10_000);\ndefer pool.deinit();\n```\n\n### acquire() !\\*Conn\nReturns a [\\*Conn](#conn) for the connection pool. Returns an `error.Timeout` if the connection cannot be acquired (i.e. if the pool remains empty) for the  `timeout` configuration passed to `init`.\n\n\n```zig\nconst conn = try pool.acquire();\ndefer pool.release(conn);\n_ = try conn.exec(\"...\", .{...});\n```\n\n### release(conn: \\*Conn) void\nReleases the conection back into the pool. Calling `pool.release(conn)` is the same as calling `conn.release()`.\n\n### newListener() !Listener\nReturns a new [Listener](#listen--notify). This function creates a new connection, it does not use/acquire a connection from the pool. It is a convenience function for cases which have already setup a pool (with the connection and authentication configuration) and want to create a listening connection using those settings.\n\n### exec / query / queryOpts / row / rowOpts\nFor single-query operations, the pool offers wrappers around the connection's `exec`, `query`, `queryOpts`, `row` and `rowOpts` methods. These are convenience methods. \n\n`pool.exec` acquires, executes and releases the connection.\n\n`pool.query` and `pool.queryOpts` acquire and execute the query. The connection is automatically returned to the pool when `result.deinit()` is called. Note that this is a special behavior of `pool.query`. When the result comes explicitly from a `conn.query`, `result.deinit()` does not automatically release the connection back into the pool.\n\n`pool.row` and `pool.rowOpts` acquire and execute the query. The connection is automatically returned to the pool when `row.deinit()` is called. Note that this is a special behavior of `pool.row`. When the result comes explicitly from a `conn.row`, `row.deinit()` does not automatically release the connection back into the pool.\n\n## Conn\n\n### open(allocator: std.mem.Allocator, opts: Opts) !Conn\nOpens a connection, or returns an error. Prefer creating connections through the pool. Connection options are:\n\n* `host` - Defaults to `\"127.0.0.1\"`\n* `port` - Defaults to `5432`\n* `write_buffer` - Size of the write buffer, used when sending messages to the server. Will temporarily allocate more space as needed. If you're writing large SQL or have large parameters (e.g. long text values), making this larger might improve performance a little. Defaults to `2048`, cannot be less than `128`.\n* `read_buffer` - Size of the read buffer, used when reading data from the server. Will temporarily allocate more space as needed. Given most apps are going to be reading rows of data, this can have large impact on performance. Defaults to `4096`.\n* `result_state_size` - Each `Result` (retrieved via a call to `query`) carries metadata about the data (e.g. the type of each column). For results with less than or equal to `result_state_size` columns, a static `state` container is used. Queries with more columns require a dynamic allocation. Defaults to `32`. \n\n### deinit(conn: \\*Conn) void\nCloses the connection and releases its resources. This method should not be used when the connection comes from the pool.\n\n### auth(opts: Opts) !void\nAuthentications the request. Prefer creating connections through the pool. Auth options are:\n\n* `username`: Defaults to `\"postgres\"`\n* `password`: Defaults to `null`\n* `database`: Defaults to `null`\n* `timeout` : Defaults to `10_000` (milliseconds)\n* `application_name`: Defaults to `null`\n* `params`: Defaults to `null`. An `std.StringHashMap([]const u8)`\n\n### release(conn: \\*Conn) void\nReleases the connection back to the pool. The pool might decide to close the connection and open a new one.\n\n### exec(sql: []const u8, args: anytype) !?usize\nExecutes the query with arguments, returns the number of rows affected, or null. Should not be used with a query that returns rows.\n\n### query(sql: []const u8, args: anytype) !Result\nExecutes the query with arguments, returns [Result](#result). `deinit`, and possibly `drain`, must be called on the returned `result`.\n\n### queryOpts(sql: []const u8, args: anytype, opts: Conn.QueryOpts) !Result\nSame as `query` but takes options:\n\n- `timeout: ?u32` - This is not reliable and should probably not be used. Currently it simply puts a recv socket timeout. On timeout, the connection will likely no longer be valid (which the pool will detect and handle when the connection is released) and the underlying query will likely still execute. Defaults to `null`\n- `column_names: bool` - Whether or not the `result.column_names` should be populated. When true, this requires memory allocation (duping the column names). Defaults to `false` unless the `column_names` build option was set to true.\n- `allocator` - The allocator to use for any allocations needed when executing the query and reading the results. When `null` this will default to the connection's allocator. If you were executing a query in a web-request and each web-request had its own arena tied to the lifetime of the request, it might make sense to use that arena. Defaults to `null`.\n- `release_conn: bool` - Whether or not to call `conn.release()` when `result.deinit()` is called. Useful for writing a function that acquires a connection from a `Pool` and returns a `Result`. When `query` or `row` are called from a `Pool` this is forced to `true`. Otherwise, defaults to `false`. \n\n### row(sql: []const u8, args: anytype) !?QueryRow\nExecutes the query with arguments, returns a single row. Returns an error if the query returns more than one row. Returns `null` if the query returns no row. `deinit` must be called on the returned `QueryRow`.\n\n### rowOpts(sql: []const u8, args: anytype, opts: Conn.QueryOpts) !Result\nSame as `row` but takes the same options as `queryOpts`\n\n### prepare(sql: []const u8) !Stmt\nCreates a [Stmt](#stmt). It is generally better to use `query`, `row` or `exec`, \n\n### prepareOpts(sql: []const u8, opts: Conn.QueryOpts) !Stmt\nSame as `prepare` but takes the same options as `queryOpts`\n\n### begin() !void\nCalls `_ = try execOpts(\"begin\", .{}, .{})`\n\n### commit() !void\nCalls `_ = try execOpts(\"commit\", .{}, .{})`\n\n### rollback() !void\nCalls `_ = try execOpts(\"rollback\", .{}, .{})`\n\n## Result\nThe `conn.query` and `conn.queryOpts` methods return a `pg.Result` which is used to read rows and values.\n\n### Fields\n* `number_of_columns: usize` - Number of columns in the result\n* `column_names: [][]const u8` - Names of the column, empty unless the query was executed with the `column_names = true` option or the `column_names` build option was set to true.\n\n### deinit(result: \\*Result) void\nReleases resources associated with the result.\n\n### drain(result: \\*Result) !void\nIf you do not iterate through the result until `next` returns `null`, you must call `drain`. \n\nWhy can't `deinit` handle this? If `deinit` also drained, you'd have to handle a possible error in `deinit` and you can't `try` in a defer. Thus, this is done to provide better ergonomics for the normal case - the normal case being where `next` is called until it returns `null`. In these cases, just `defer result.deinit()`.\n\n### next(result: \\*Result) !?Row\nIterates to the next row of the result, or returns null if there are no more rows.\n\n### columnIndex(result: \\*Result, name: []const u8) ?usize\nReturns the index of the column with the given name. This is only valid when the query is executed with the `column_names = true` option or the `column_names` build option was set to true.\n\n### mapper(result: \\*Result, T: type, opts: MapperOpts) Mapper(T)\nReturns a Mapper which can be used to create a T for each row. Mapping from column to field is done by name. This is an optimized version of [row.to](#tot-type-opts-toopts-t) when iterating through multiple rows with the `{.map = .name}`.\n\nSee [row.to](#tot-type-opts-toopts-t) and [Mapper](#mapper) for more information.\n\n## Row\nThe `row` represents a single row from a result. Any non-primitive value that you get from the `row` are valid only until the next call to `next`, `deinit` or `drain`.\n\n### Fields\nOnly advance usage will need access to the row fields:\n\n* `oids: []i32` - The PG OID value for each column in the row. See `result.number_of_columns` for the length of this slice. Might be useful if you're trying to read a non-natively supported type.\n* `values: []Value` - The underlying byte value for each column in the row.  See `result.number_of_columns` for the length of this slice. Might be useful if you're trying to read a non-natively supported type. Has two fields, `is_null: bool` and `data: []const u8`.\n\n### get(comptime T: type, col: usize) T\nGets a value from the row at the specified column index (0-based). **Type mapping is strict.** For example, you **cannot** use `i32` to read an `smallint` column.\n\nFor any supported type, you can use an optional instead. Therefore, if you use `row.get(i16, 0)` the return type is `i16`. If you use `row.get(?i16, 0)` the return type is `?i16`. If you use a non-optional type for a null value, you'll get a failed assertion in `Debug` and `ReleaseSafe`, and undefined behavior in `ReleaseFast`, `ReleaseSmall` or if you set `pg_assert = false`.\n\n* `u8` - `char`\n* `i16` - `smallint`\n* `i32` - `int`\n* `i64` - Depends on the underlying column type. A `timestamp(tz)` will be converted to microseconds since unix epoch. Otherwise, a `bigint`.\n* `f32` - `float4`\n* `f64` - Depends on the underlying column type. A `numeric` will be converted to an `f64`. Otherwise, a `float`.\n* `bool` - `bool`\n* `[]const u8` - Returns the raw underlying data. Can be used for any column type to get the PG-encoded value. For `text` and `bytea` columns, this will be the expected value. For `numeric`, this will be a text representation of the number. For `UUID` this will be a 16-byte slice (use `pg.uuidToHex [36]u8` if you want a hex-encoded UUID). For `JSON` and `JSONB` this will be the serialized JSON value.\n* `[]u8` - Same as []const u8 but returns a mutable value.\n* `pg.Numeric` - See numeric section\n* `pg.Cidr` - See CIDR/INET section\n\n### getCol(comptime T: type, column_name: []const u8) T\nSame as `get` but uses the column name rather than its position. Only valid when the `column_names = true` option is passed to `queryOpts` or the `column_names` build option was set to true.\n\nThis relies on calling `result.columnIndex` which iterates through `result.column_names` fields. In some cases, this is more efficient than `StringHashMap` lookup, in others, it is worse. For performance-sensitive code, prefer using `get`, or cache the column index in a local variables outside of the `next()` loop:\n\n```zig\nconst id_idx = result.columnIndex(\"id\").?\nwhile (try result.next()) |row| {\n  // row.get(i32, id_idx)\n}\n```\n\n### Array Columns\nUse `row.get(pg.Iterator(i32))` to return an [Iterator](#iteratort) over an array column. Supported array types are:\n\n* `u8` and `?u8` - `char[]`\n* `i16` and `?i16` - `smallint[]`\n* `i32` and `?i32` - `int[]`\n* `i64` and `?i64` - `bigint[]` or `timestamp(tz)[]` (see `get`)\n* `f32` and `?f32` - `float4`\n* `f64` and `?f64` - `float8`\n* `bool` and `?bool` - `bool[]`\n* `[]const u8` and `[]?const u8` - More strict than `get([]u8)`). Supports: `text[]`, `char(n)[]`, `bytea[]`, `uuid[]`, `json[]` and `jsonb[]`\n* `[]u8` - Same as `[]const u8` but returns mutable value.\n* `pg.Numeric` - See numeric section\n* `pg.Cidr` - See CIDR/INET section\n\n### record(col: usize) Record\nGets a [Record](#record) by column position.\n\n### recordCol(column_name: []const u8) Record\nGets an [Record](#record) by column name. See [getCol](#getcolcomptime-t-type-column_name-const-u8-t) for performance notes.\n\n### to(T: type, opts: ToOpts) !T\nPopulates and returns a `T`. \n\n`opts` values are:\n* `dupe` - Duplicate string columns using the internal arena. When set to `true` non-scalar values are valid until `deinit` is called on the `row`/`result`. Defaults to `false`\n* `allocator` - Allocator to use to duplicate non-scalar values (i.e. strings). It is the caller's responsible to free any non-scalar values from their structure. Defaults to `null`.\n* `map` - `.ordinal` or `.name`, defaults to `.ordinal`\n\nSetting `allocator` implies `dupe`, but uses the specified allocator rather than the internal arena. By default (when `dupe` is `false` and `allocator` is `null`), non-scalar values (i.e. strings) are only valid until the next call to `next()` or `drain()` or `deinit()`.\n\nWhen `.map = .ordinal`, the default, the order of the field names must match the order of the columns. \n\nWhen `.map = .name`, the query must be executed with the  `{.column_names = true}` option or the `column_names` build option must be set. Columns with no field equivalent are ignored. Fields with no column equivalent are set to their default value; if they do not have a default value the function will return `error.FieldColumnMismatch`. If you're going to use this in a loop with a `result`, consider using a [Mapper](#mapper) to avoid the name->index lookup on each iteration.\n\nSlice fields can either be mapped to a `pg.Iterator(T)` or a slice. When mapped to a `slice`, an allocator MUST be provided. When mapping to an array of strings (i.e. [][]const u8), the values are duped, and thus both the values and the slice itself must be freed. When mapping to a slice of primitives (i.e. []i32) the slice must be freed. When mapping to an `pg.Iterator(T)` with a custom allocator (`.{.allocator = allocator}`), the iterator must be freed by calling `iteartor.deinit(allocator)`. Whether you're mapping to an `pg.Iterator(T)` or a slice, I Strongly suggest you use an ArenaAllocator.\n\n## QueryRow\nA `QueryRow` is returned from a call to `conn.row` or `conn.rowOpts` and wraps both a `Result` and a `Row.` It exposes the same methods as `Row` as well as `deinit`, which must be called once the `QueryRow` is no longer needed. This is a rare case where `deinit()` can fail. In most cases, you can simply throw away the error (because failure is extremely rare and, if the connection came from a pool, it should repair itself).\n\n## Iterator(T)\nThe iterator returned from `row.get(pg.Iterator(T), col)` can be iterated using the `next() ?T` call:\n\n```zig\nvar names = row.get(pg.Iterator([]const u8), 0);\nwhile (names.next()) |name| {\n  ...\n}\n```\n\n### Fields\n* `len` - the number of values in the iterator\n* `is_null` - Whether the array itself was null\n\n### alloc(it: Iterator(T), allocator: std.mem.Allocator) ![]T\nAllocates a slice and populates it with all values. \n\nIf the slice is a `[]u8` or `[]const u8`, the string is also duplicated. It is the responsibility of the caller to free the string values AND the slice.\n\n### fill(it: Iterator(T), into: []T) void\nFill `into` with values of the iterator. `into` can be smaller than `it.len`, in which case only `into.len` values will be filled. This can be a bit faster than calling `next()` multiple times. Values are not duplicated; they are only valid until the next iterations.\n\n## Record\nReturned by `row.record(col)` for fetching a PostgreSQL record-type, for example from this query: \n\n```sql\nselect row('over', 9000)\n```\n\nIn many cases, PostgreSQL will mark the inner-types as \"unknown\", which is likely to cause assertion failures in this library. The solution is to type each value:\n\n```sql\nselect row('over'::text, 9000::int)\n```\n\n### Fields\n* `number_of_columns` - the number of columns in the record\n\n### next(T) T\nGets the next column in the record. This behaves similarly [row.get](#getcomptime-t-type-col-usize-t) with the same supported types for `T`, including nullables.\n\n## Mapper\nA mapper is used to iterate through a result and turn a row into an instance of `T`. When converting a single row, or using ordinal mapping, prefer using [row.to](#tot-type-opts-toopts-t). The mapper is an optimization over `row.to` with the `{.map = .name}` option which only has to do the name -> index lookup once.\n\nTo use a mapper, the `{.column_names = true}` option must be passed to the query/row function or the `column_names` build option must be set.\n\n```zig\nconst User = struct {\n  id: i32,\n  name: []const u8,\n};\n\n///...\n\nvar result = try conn.queryOpts(\"select id, name from users\", .{}, .{.column_names = true});\ndefer result.deinit();\n\nvar mapper = result.mapper(User, .{});\nwhile (try mapper.next()) |user| {\n  // use: user.id and user.name\n}\n```\n\nA column with no matching field is ignored. A field with no matching column is set to its default fault. If no default value is defined, `mapper.next()` will return `error.FieldColumnMismatch`.\n\nThe 2nd argument to `result.mapper` is an option:\n\n* `dupe` - Duplicate string columns using the internal arena. When set to `true` non-scalar values are valid until `deinit` is called on the `row`/`result`. Defaults to `false`\n* `allocator` - Allocator to use to duplicate non-scalar values (i.e. strings). It is the caller's responsible to free any non-scalar values from their structure. Defaults to `null`.\n\nSetting `allocator` implies `dupe`, but uses the specified allocator rather than the internal arena. By default (when `dupe` is `false` and `allocator` is `null`), non-scalar values (i.e. strings) are only valid until the next call to `next()` or `drain()` or `deinit()`.\n\n## Stmt\nFor most queries, you should use the `conn.query(...)`, `conn.row(...)` or `conn.exec(...)` methods. For queries with parameters, these methods look like:\n\n```zig\nvar stmt = try Stmt.init(conn, opts);\nerrdefer stmt.deinit();\n\ntry stmt.prepare(sql, null);\ninline for (parameters) |param| {\n  try stmt.bind(param);\n}\n\nreturn stmt.execute();\n```\n\nYou can create a statement directly using `conn.prepare(sql)` or `conn.prepareOpts(sql, ConnQueryOpts{...})` and call `stmt.bind(value: anytype)` and `execute()` directly.\n\nThe main reason to do this is to have more flexibility in binding parameters (e.g. such as when creating dynanmic SQL where all the parameters aren't fixed at compile-time).\n\nNote that `stmt.deinit()` should only be called if `stmt.execute()` is not called or returns an error. Once `stmt.execute()` returns a [Result](#result), `stmt` should be considered invalid. As we can see in the above example, `stmt.deinit()` is only called on `errdefer`.\n\n## Caching Prepared Statements\nWhen you execute a statement with parameters, we first ask PostgreSQL to \"parse\" the statement (which creates an execution plan) and then describe it. We can then bind the parameters and execute the statement.\n\nIf you plan on executing the same query(ies) repeatedly, it's possible to have PostgreSQL cache the execution plan and pg.zig cache the description. However, there are some caveats with this approach (which are not specific to pg.zig). First, if you're using a connection pooler (like pgpool or PgBouncer), make sure to read the documentation and configure it to work properly with cached prepared statements. Historically, cached prepared statements and connection poolers have not worked well together.\n\nSecondly, note that the cache is per-connection. If you have a pool of 50 connections, and you execute the query against connections from the pool, then you should expect the full parse -> describe -> bind -> execute flow for those 50 connections (plus whatever new connections the pool might open).\n\nCaching is enabled by passing the `cache_name` option:\n\n```zig\nconst result = try conn.queryOpts(\n    \"select * from saiyans where power > $1\", \n    .{9000}, \n    .{.cache_name = \"super\"}\n);\n```\n\nTechnically, once cached, the SQL statement is ignored. So, after executing the above, you could execute the following **on the same connection**:\n\n```zig\nconst result = try conn.queryOpts(\n    \"this isn't valid SQL\", \n    .{1000}, \n    .{.cache_name = \"super\"}\n);\n```\n\nAnd it **will** work. But you're playing with fire, and you should just include the same SQL and the same cache name for each execution. If you want to use the `.column_names = true` option, then it _must_ be included in the first query which generated the cache entry (again, in short, just _always_ use the same SQL and the same options).\n\nYou can call `try conn.deallocate(\"super\")` to remove a cache entry. But this is only done for the connection on which it is called. This would make sense, for example, if you get a connection from the pool, execute the same query multiple times, deallocate the cached entry, and return the connection back to the pool. Note that the name to deallocate, `super`, is not sanitized and is open to SQL injection - don't pass a user-supplied value to `deallocte`.\n\n## Important Notice 1 - Bind vs Read\nWhen you read a value, such as `row.get(i32, 0)`, the library assumes you know what you're doing and that column 0 really is a non-null 32-bit integer. `row.get` doesn't return an error union. There are some assertions, but these are disabled in ReleaseFast and ReleaseSmall. You can also disable these assertions in Debug/ReleaseSafe by placing `pub const pg_assert = false;` in your root, (e.g. `main.zig`):\n\n```zig\nconst std = @import(\"std\");\n...\n\npub const pg_assert = false;\n\npub fm main() !void {\n  ...\n}\n```\n\nConversely, when binding a value to an SQL parameter, the library is a little more generous. For example, an `u64` will bind to an `i32` provided the value is within range.\n\nThis is particularly relevant for types which are expressed as `[]u8`. For example a UUID can be a raw binary `[16]u8` or a hex-encoded `[36]u8`. Where possible (e.g. UUID, MacAddr, MacAddr8), the library will support binding either the raw binary data or text-representation. When reading, the raw binary value is always returned.\n\n## Important Notice 2 - Invalid Connections\nStrongly consider using `pg.Pool` rather than using `pg.Conn` directly. The pool will attempt to reconnect disconnected connections or connections which are in an invalid state. Until more real world testing is done, you should assume that connections will get into invalid states.\n\n## Important Notice 3 - Errors\nZig errorsets do not support arbitrary payloads. This is problematic in a database driver where most applications probably care about the details of an error. The library takes a simple approach. If `error.PG` is returned, `conn.err` should be set and will contains a PG error object:\n\n```zig\n_ = conn.exec(\"drop table x\", .{}) catch |err| {\n  if (err == error.PG) {\n    if (conn.err) |pge| {\n      std.log.err(\"PG {s}\\n\", .{pge.message});\n    }\n  }\n  return err;\n};\n```\n\nIn the above snippet, it's possible to skip the `if (err == error.PG)` check, but in that case `conn.err` could be set from some previous command (`conn.err` is always reset when acquired from the pool).\n\nIf `error.PG` is returned from a non-connection object, like a query result, the associated connection will have its `conn.err` set. In other words, `conn.err` is the only thing you ever have to check.\n\nA PG error always exposes the following fields:\n* `code: []const u8` - https://www.postgresql.org/docs/current/errcodes-appendix.html\n* `message: []const u8`\n* `severity: []const u8`\n\nAnd optionally (depending on the error and the version of the server):\n* `column: ?[]const u8 = null`\n* `constraint: ?[]const u8 = null`\n* `data_type_name: ?[]const u8 = null`\n* `detail: ?[]const u8 = null`\n* `file: ?[]const u8 = null`\n* `hint: ?[]const u8 = null`\n* `internal_position: ?[]const u8 = null`\n* `internal_query: ?[]const u8 = null`\n* `line: ?[]const u8 = null`\n* `position: ?[]const u8 = null`\n* `routine: ?[]const u8 = null`\n* `schema: ?[]const u8 = null`\n* `severity2: ?[]const u8 = null`\n* `table: ?[]const u8 = null`\n* `where: ?[]const u8 = null`\n\nThe `isUnique() bool` method can be called on the error to determine whether or not the error was a unique violation (i.e. error code `23505`).\n\n## Type Support\nAll implementations have to deal with things like: how to support unsigned integers, given that PostgreSQL only has signed integers. Or, how to support UUIDs when the language has no UUID type. This section documents the exact behavior.\n\n### Arrays\nMulti-dimensional arrays aren't supported. The array lower bound is always 0 (or 1 in PG)\n\n### text, bool, bytea, char, char(n), custom enums\nNo surprises, arrays supported. \n\nWhen reading a `char[]`, it's tempting to use `row.get([]u8, 0)`, but this is incorrect. A `char[]` is an array, and thus `row.get(pg.Iterator(u8), 0`) must be used.\n\n### smallint, int, bigint\nWhen binding an integer, the library will coerce the Zig value to the parameter type, as long as it fits. Thus, a `u64` can be bound to a `smallint`, if the value fits, else an error will be returned.\n\nArray binding is strict. For example, an `[]i16` must be used for a `smallint[]`parameter. The only exception is that the unsigned variant, e.g. `[]u16` can be used provided all values fit.\n\nWhen reading a column, you must use the correct type. \n\n### Floats\nWhen binding, `@floatCast` is used based on the SQL parameter type. Array binding is strict. When reading a value, you must use the correct type. \n\n### Numeric\nUntil standard support comes to Zig (either in the stdlib or a de facto standard library), numeric support is half-baked. When binding a value to a parameter, you can use a f32, f64, comptime_float or string. The same applies to binding to a numeric array.\n\nYou can `get(pg.Numeric, $COL)` to return a `pg.Numeric`. The `pg.Numeric` type only has 2 useful methods: `toFloat` and `toString`. You can also use `num.estimatedStringLen` to get the max size of the string representation:\n\n```zig\nconst numeric = row.get(pg.Numeric, 0);\nvar buf = allocator.alloc(u8, numeric.estimatedStringLen());\ndefer allocator.free(buf)\nconst str = numeric.toString(&buf);\n```\n\nUsing `row.get(f64, 0)` on a numeric is the same as `row.get(pg.Numeric, 0).toFloat()`.\n\nYou should consider simply casting the numeric to `::double` or `::text` within SQL in order to rely on PostgreSQL's own robust numeric to float/text conversion.\n\nHowever, `pg.Numeric` has fields for the underlying wire-format of the numeric value. So if you require precision and the text representation isn't sufficient, you can parse the fields directly. `types/numeric.zig` is relatively well documented and tries to explain the fields. Note that any non-primitive fields, e.g. the  `digits: []u8`, is only valid until the next call to `result.next`, `result.deinit`, `result.drain` or `row.deinit`.\n\n### UUID\nWhen a `[]u8` is bound to a UUID column, it must either be a 16-byte slice, or a valid 36-byte hex-encoded UUID. Arrays behave the same.\n\nWhen reading a `uuid` column with `[]u8` a 16-byte slice will be returned. Use the `pg.uuidToHex() ![36]u8` helper if you need it hex-encoded.\n\n### INET/CIDR\nYou can bind a string value to a `cidr`, `inet`, `cidr[]` or `inet[]` parameter.\n\nWhen reading a value, via `row.get` or `row.iterator` you should use `pg.Cidr`. It exposes 3 fields:\n\n* `address: []u8` - Will be a 4 or 16 byte slice depending on the family\n* `family: Family` - An enum, either `Family.v4` of `Family.v6`\n* `netmask: u8` - The network mask\n\n### MacAddr/MacAddr8\nYou can bind a `[]u8` to either a `macaddr` or a `macaddr8`. These can be either binary representation (6-bytes for `macaddr` or 8 bytes for `macaddr8`) or a text-representation supported by PostgreSQL. This works, like UUID, because there's no ambiguity in the length. The same applied for array variants - it's even possible to mix and match formats within the array.\n\nWhen reading a value, via `row.get` or `row.iterator` using `[]u8`, the binary representation is always returned.\n\n### Timestamp(tz)\nWhen you bind an `i64` to a timestamp(tz) parameter, the value is assumed to be the number of microseconds since unix epoch (e.g. `std.time.microTimestamp()`). Array binding works the same. You can also bind a string, which will pass the string as-is and depend on PostgreSQL to do the conversion. This is true for arrays as well.\n\nWhen reading a `timestamp` column with `i64`, the number of microseconds since unix epoch will be returned\n\n### JSON and JSONB\nWhen binding a value to a JSON or JSONB parameter, you can either supply a serialized value (i.e. `[]u8`) or a struct which will be serialized using `std.json.stringify`.\n\nWhen binding to an array of JSON or JSONB, automatic serialization is not support and thus an array of serialized values must be provided.\n\nWhen reading a `JSON` or `JSONB` column with `[]u8`, the serialized JSON will be returned.\n\n### PgLSN, xid8, xid\nPgLSN and xid8 can be bound and read as i64.\n\nxid can be bound and read as i32.\n\n### Arbitrary Binary Encoding\nFor other types, either open an issue (ideally, with a sample query/data), or you can use binary encoding directly. \n\nFor reading, you can use `[]u8` to get the raw binary encoded data and parse it yourself.\n\nFor writing, wrap your raw encoded data in `pg.Binary{.data = ....}`.\n\n## Listen / Notify\nYou can create a `pg.Listener` either from an existing `Pool` or directly.\n\nCreating a new Listener directly is a lot like creating a new connection. See [Conn.open](#openallocator-stdmemallocator-opts-opts-conn) and [Conn.auth](#authopts-opts-void).\n\n```zig\n// see the Conn.ConnectOpts\nvar listener = try pg.Listener.open(allocator, .{\n  .host = \"127.0.0.1\",\n  .port = 5432,\n});\ndefer listener.deinit();\n\ntry listener.auth(.{\n  .username = \"leto\",\n  .password = \"ghanima\",\n  .database = \"caladan\",\n});\n\n// add 1 or more channels to listen to\ntry listener.listen(\"chan_1\", .{});\ntry listener.listen(\"chan_2\", .{});\n\n// .next() blocks until there's a notification or an error\nwhile (listener.next()) |notification| {\n  std.debug.print(\"Channel: {s}\\nPayload: {s}\", .{notification.channel, notification.payload});\n}\n\n// The error handling is explained, sorry about this API. Zig error payloads plz\nswitch (listener.err.?) {\n  .pg => |pg| std.debug.print(\"{s}\\n\", .{pg.message}),\n  .err => |err| std.debug.print(\"{s}\\n\", .{@errorName(err)}),\n}\n```\n\nWhen using the pool, a new connection/session is created. It *does not* use a connection from the pool. This is merely a convenience function if you're also using normal connections through a pool.\n\n```zig\nvar listener = try pool.newListener();\ndefer listener.deinit();\n\n// listen to 1 or more channels\ntry listener.listen(\"chan_1\");\n\n// same as above\n```\n\n### Listen Timeout\nWhen calling `listen`, a timeout in milliseconds can be specified:\n\n```zig\ntry listener.listen(\"chan_1\", .{});\n```\n\nIf multiple calls to `listen` are made, the last timeout will be used. If no message is received in `timeout` milliseconds, `next()` will return `null` and `listener.err.?.err == error.WouldBlock`.\n\n### Reconnects\nA listener will not automatically reconnect on error/disconnect. The pub/sub nature of LISTEN/NOTIFY mean that delivery is at-most-once and auto-reconnecting can hide that fact. Put the above code in a `while (true) {...}` loop.\n\n### Stop\nIt is safe to call `listener.stop()` from a different thread. When called, `next()` will return `null` and `listener.stopped` will be `true`.\n\n## Errors\nThe handling of errors isn't great. Blame Zig's lack of error payloads and the awkwardness of using `try` within a `while` condition.\n\n`listener.next()` can only return `null` on error. When `null` is returned, `listener.err` will be non-null. Unlike the `Conn` this is a tagged union that can either be `err` for a normal Zig error (e.g. error.ConnectionResetByPeer) or `pg` a detailed PostgresSQL error.\n\n## Metrics\nA few basic metrics are collected using [metrics.zig](https://github.com/karlseguin/metrics.zig), a prometheus-compatible library. These can be written to an `std.io.Writer` using `try pg.writeMetrics(writer)`. As an example using [httpz](https://github.com/karlseguin/http.zig):\n\n```zig\npub fn metrics(_: *httpz.Request, res: *httpz.Response) !void {\n    const writer = res.writer();\n    try pg.writeMetrics(writer);\n\n    // also write out the httpz metrics\n    try httpz.writeMetrics(writer);\n}\n```\n\nThe metrics are:\n\n* `pg_queries` - counts the number of queries\n* `pg_pool_empty` - counts how often the pool is empty\n* `pg_pool_dirty` - counts how often a connection is released back into the pool in an unclean state (thus requiring the connection to be closed and the pool to re-open another connection). This could indicate that results aren't being fully drained (either by calling `next()` until `null` is returned or explicitly calling the `drain()` method)\n* `pg_alloc_params` - counts the number of parameter states that were allocated. This indicates that your queries have more parameters than `result_state_size`. If this happens often, consider increasing `result_state_size`.\n* `pg_alloc_columns` - counts the number of columns states that were allocated. This indicates that your queries are returning more columns than `result_state_size`. If this happens often, consider increasing `result_state_size`.\n* `pg_alloc_reader` - counts the number of bytes allocated while reading messages from PostgreSQL. This generally happens as a result of large result (e.g. selecting large text fields). Controlled by the `read_buffer` configuration option.\n\n## TLS (Experimental)\nTLS is supported via openssl. When loading the module, you must enable openssl by including at least 1 openssl setting:\n\n```zig\nconst pg_module = b.dependency(\"pg\", .{\n  .target = target,\n  .optimize = optimize,\n  .openssl_lib_name = @as([]const u8, \"ssl\"),\n  .openssl_lib_path = std.Build.LazyPath{.cwd_relative = \"/path/to/openssl/lib\"},\n  .openssl_include_path = std.Build.LazyPath{.cwd_relative = \"/path/to/openssl/include\"},\n}).module(\"pg\")\n```\n\nWhen not specified, the system defaults are use for the library and include paths. These should only be set if openssl is installed in a non-default location. In most cases specifying `.openssl_lib_name = \"ssl\"` or, for some systems `.openssl_lib_name = \"openssl\"` should be enough.\n\nSet the connection's `tls` option to either `.required` or `.{verify_full = null}`. When using a custom root certificate, specify the path: `.{verify_full = \"/path/to/root.crt\"}`.\n\n```zig\nvar pool = try pg.Pool.init(allocator, .{\n  .connect = .{ .port = 5432, .host = \"ip_or_hostname\", .tls = .{.verify_full = null}},\n  .auth = .{ .... },\n  .size = 5,\n});\n\n// OR\nconst uri = try std.Uri.parse(\"postgresql://user:password@hostname/DBNAME?sslmode=require\");\nvar pool = try pg.Pool.initUri(allocator, uri, 10, 5_000);\n```\n\nIn your main file, you can define a global `pub const pg_stderr_tls = true;` to have pg.zig print possible TLS-related errors to stderr. Alternatively, if you get an error, you `pg.printSSLError();` to hopefully print an error message to stderr which can be included in a ticket. This can safely be called in a `catch` clause, and will display nothing if the error is NOT SSL-related. Note that using the global `pg_stderr_tls` is more likely to print useful information in the case of certification verification problems.\n\n## Enabling Column Names by Default\n\nTo execute all queries as if the `column_names` option was set to true, you can provide the `column_names` argument to `b.dependency()`:\n\n```zig\nconst pg_module = b.dependency(\"pg\", .{\n  .target = target,\n  .optimize = optimize,\n  .column_names = true,\n}).module(\"pg\");\n```\n\n## Tests\n\nLaunch the Postgres database with the provided Docker Compose configuration:\n\n```console\ncd tests/\ndocker compose up\n```\n\nRun tests:\n\n```console\nzig build test\n```\n",
  "owner_avatar_url": "https://avatars.githubusercontent.com/u/206480?v=4",
  "releases": [],
  "owner_company": null,
  "owner_location": null,
  "owner_blog": "https://www.openmymind.net/",
  "owner_twitter_username": "karlseguin",
  "owner_followers": 2253,
  "owner_following": 3,
  "owner_created_at": "2010-02-19T04:01:58Z",
  "license": "MIT",
  "category": "library"
}